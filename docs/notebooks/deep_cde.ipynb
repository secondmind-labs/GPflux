{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61537fc9",
   "metadata": {},
   "source": [
    "# Deep Gaussian processes with Latent Variables\n",
    "\n",
    "In this notebook, we explore the use of Deep Gaussian processes <cite data-cite=\"damianou2013deep\"/> and Latent Variables to model a dataset with heteroscedastic noise. The model can be seen as a deep GP version of <cite data-cite=\"dutordoir2018cde\"/> or as doing variational inference in models from <cite data-cite=\"salimbeni2019iwvi\"/>. We start by fitting a single layer GP model to show that it doesn't result in a satisfactory fit for the noise.\n",
    "\n",
    "This notebook is inspired by [prof. Neil Lawrence's Deep Gaussian process talk](https://inverseprobability.com/talks/notes/deep-gps.html), which we highly recommend watching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gpflow\n",
    "import gpflux\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.neighbors import KernelDensity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bccd7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Load data\n",
    "\n",
    "The data comes from a motorcycle accident simulation [1] and shows some interesting behaviour. In particular the heteroscedastic nature of the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bbc183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motorcycle_data():\n",
    "    \"\"\"Return inputs and outputs for the motorcycle dataset. We normalise the outputs.\"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(\"./data/motor.csv\", index_col=0)\n",
    "    X, Y = df[\"times\"].values.reshape(-1, 1), df[\"accel\"].values.reshape(-1, 1)\n",
    "    Y = (Y - Y.mean()) / Y.std()\n",
    "    X /= X.max()\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = motorcycle_data()\n",
    "num_data, d_xim = X.shape\n",
    "\n",
    "X_MARGIN, Y_MARGIN = 0.1, 0.5\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y, marker=\"x\", color=\"k\")\n",
    "ax.set_ylim(Y.min() - Y_MARGIN, Y.max() + Y_MARGIN)\n",
    "ax.set_xlim(X.min() - X_MARGIN, X.max() + X_MARGIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40b3e1",
   "metadata": {},
   "source": [
    "## Standard single layer Sparse Variational GP\n",
    "\n",
    "We first show that a single layer SVGP performs quite poorly on this dataset. In the following code block we define the kernel, inducing variable, GP layer and likelihood of the shallow GP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4e5a8",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "NUM_INDUCING = 20\n",
    "\n",
    "kernel = gpflow.kernels.SquaredExponential()\n",
    "inducing_variable = gpflow.inducing_variables.InducingPoints(\n",
    "    np.linspace(X.min(), X.max(), NUM_INDUCING).reshape(-1, 1)\n",
    ")\n",
    "gp_layer = gpflux.layers.GPLayer(kernel, inducing_variable, num_data=num_data, num_latent_gps=1)\n",
    "likelihood_layer = gpflux.layers.LikelihoodLayer(gpflow.likelihoods.Gaussian(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914574b",
   "metadata": {},
   "source": [
    "We can now encapsulate `gp_layer` in a GPflux DeepGP model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2471c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "single_layer_dgp = gpflux.models.DeepGP([gp_layer], likelihood_layer)\n",
    "model = single_layer_dgp.as_training_model()\n",
    "model.compile(tf.optimizers.Adam(0.01))\n",
    "\n",
    "history = model.fit({\"inputs\": X, \"targets\": Y}, epochs=int(1e3), verbose=0)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(history.history[\"loss\"])\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa743e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "num_data_test = 200\n",
    "X_test = np.linspace(X.min() - X_MARGIN, X.max() + X_MARGIN, num_data_test).reshape(-1, 1)\n",
    "model = single_layer_dgp.as_prediction_model()\n",
    "out = model(X_test)\n",
    "\n",
    "mu = out.y_mean.numpy().squeeze()\n",
    "var = out.y_var.numpy().squeeze()\n",
    "X_test = X_test.squeeze()\n",
    "\n",
    "for i in [1, 2]:\n",
    "    lower = mu - i * np.sqrt(var)\n",
    "    upper = mu + i * np.sqrt(var)\n",
    "    ax.fill_between(X_test, lower, upper, color=\"C1\", alpha=0.3)\n",
    "\n",
    "ax.set_ylim(Y.min() - Y_MARGIN, Y.max() + Y_MARGIN)\n",
    "ax.set_xlim(X.min() - X_MARGIN, X.max() + X_MARGIN)\n",
    "ax.plot(X, Y, \"kx\", alpha=0.5)\n",
    "ax.plot(X_test, mu, \"C1\")\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_ylabel(\"acc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c1a4e",
   "metadata": {},
   "source": [
    "The errorbars of the single layer model are not good: we observe an overestimation of the error bars on the left and right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb254c",
   "metadata": {},
   "source": [
    "## Deep Gaussian process with latent variables\n",
    "\n",
    "To tackle the problem we suggest a Deep Gaussian process with a latent variable in the first layer. The latent variable will be able to capture the\n",
    "heteroscedasticity, while the two-layered deep GP is able to model the sharp transitions.\n",
    "\n",
    "Note that a GPflux Deep Gaussian process by itself (i.e. without the latent variable layer) is not able to capture the heteroscedasticity of this dataset. This is a consequence of the noise-less hidden layers and the doubly-stochastic variational inference training procedure, as forumated in <cite data-cite=\"salimbeni2017doubly\">. On the contrary, the original deep GP suggested by Damianou and Lawrence <cite data-cite=\"damianou2013deep\">, using a different variational approximation for training, can model this dataset without a latent variable, as shown in [this blogpost](https://inverseprobability.com/talks/notes/deep-gps.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce6ea5",
   "metadata": {},
   "source": [
    "### Latent Variable Layer\n",
    "\n",
    "This layer concatenates the inputs with a latent variable. See Dutordoir, Salimbeni et al. Conditional Density with Gaussian processes (2018) <cite data-cite=\"dutordoir2018cde\"/> for full details. We choose a one-dimensional input and a full parameterisation for the latent variables. This means that we do not need to train a recognition network, which is useful for fitting but can only be done in the case of small datasets, as is the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6cf7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dim = 1\n",
    "prior_means = np.zeros(w_dim)\n",
    "prior_std = np.ones(w_dim)\n",
    "encoder = gpflux.encoders.DirectlyParameterizedNormalDiag(num_data, w_dim)\n",
    "prior = tfp.distributions.MultivariateNormalDiag(prior_means, prior_std)\n",
    "lv = gpflux.layers.LatentVariableLayer(prior, encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b7798",
   "metadata": {},
   "source": [
    "### First GP layer\n",
    "\n",
    "GP Layer with two dimensional input because it acts on the inputs and the one-dimensional latent variable. We use a Squared Exponential kernel, a zero mean function, and inducing points, whose pseudo input locations are carefully chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kernel = gpflow.kernels.SquaredExponential(lengthscales=[0.05, 0.2], variance=1.0)\n",
    "inducing_variable = gpflow.inducing_variables.InducingPoints(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            np.linspace(X.min(), X.max(), NUM_INDUCING).reshape(-1, 1),\n",
    "            np.random.randn(NUM_INDUCING, 1),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    ")\n",
    "gp_layer = gpflux.layers.GPLayer(\n",
    "    kernel,\n",
    "    inducing_variable,\n",
    "    num_data=num_data,\n",
    "    num_latent_gps=1,\n",
    "    mean_function=gpflow.mean_functions.Zero(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93fe2e1",
   "metadata": {},
   "source": [
    "### Second GP layer\n",
    "\n",
    "Final layer GP with Squared Exponential kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ef693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kernel = gpflow.kernels.SquaredExponential()\n",
    "inducing_variable = gpflow.inducing_variables.InducingPoints(\n",
    "    np.random.randn(NUM_INDUCING, 1),\n",
    ")\n",
    "gp_layer2 = gpflux.layers.GPLayer(\n",
    "    kernel,\n",
    "    inducing_variable,\n",
    "    num_data=num_data,\n",
    "    num_latent_gps=1,\n",
    "    mean_function=gpflow.mean_functions.Identity(),\n",
    ")\n",
    "gp_layer2.q_sqrt.assign(gp_layer.q_sqrt * 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c383ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "likelihood_layer = gpflux.layers.LikelihoodLayer(gpflow.likelihoods.Gaussian(0.01))\n",
    "gpflow.set_trainable(likelihood_layer, False)\n",
    "dgp = gpflux.models.DeepGP([lv, gp_layer, gp_layer2], likelihood_layer)\n",
    "gpflow.utilities.print_summary(dgp, fmt=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36132b7d",
   "metadata": {},
   "source": [
    "### Fit\n",
    "\n",
    "We can now fit the model. Because of the `DirectlyParameterizedEncoder` it is important to set the batch size to the number of datapoints and turn off shuffle. This is so that we use the associated latent variable for each datapoint. If we would use an amortized encoder network this would not be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f21ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dgp.as_training_model()\n",
    "model.compile(tf.optimizers.Adam(0.005))\n",
    "history = model.fit(\n",
    "    {\"inputs\": X, \"targets\": Y}, epochs=int(20e3), verbose=0, batch_size=num_data, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpflow.utilities.print_summary(dgp, fmt=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6a9f1",
   "metadata": {},
   "source": [
    "### Prediction and plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6359dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.linspace(X.min() - X_MARGIN, X.max() + X_MARGIN, num_data_test).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def predict_y_samples(prediction_model, Xs, num_samples=25):\n",
    "    samples = []\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        out = prediction_model(Xs)\n",
    "        s = out.y_mean + out.y_var ** 0.5 * tf.random.normal(\n",
    "            tf.shape(out.y_mean), dtype=out.y_mean.dtype\n",
    "        )\n",
    "        samples.append(s)\n",
    "    return tf.concat(samples, axis=1)\n",
    "\n",
    "\n",
    "def plot_samples(ax, N_samples=25):\n",
    "    samples = predict_y_samples(dgp.as_prediction_model(), Xs, N_samples).numpy().T\n",
    "    Xs_tiled = np.tile(Xs, [N_samples, 1])\n",
    "    ax.scatter(Xs_tiled.flatten(), samples.flatten(), marker=\".\", alpha=0.2, color=\"C0\")\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.set_xlim(min(Xs), max(Xs))\n",
    "    ax.scatter(X, Y, marker=\".\", color=\"C1\")\n",
    "\n",
    "\n",
    "def plot_latent_variables(ax):\n",
    "    for l in dgp.f_layers:\n",
    "        if isinstance(l, gpflux.layers.LatentVariableLayer):\n",
    "            m = l.encoder.means.numpy()\n",
    "            s = l.encoder.stds.numpy()\n",
    "            ax.errorbar(X.flatten(), m.flatten(), yerr=s.flatten(), fmt=\"o\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27137aaf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c63a6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "plot_samples(ax1)\n",
    "plot_latent_variables(ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed058c5d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Left we show the dataset and posterior samples of $y$. On the right we plot the mean and std. deviation of the latent variables corresponding to the datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_and_var(ax, samples=None, N_samples=5_000):\n",
    "    if samples is None:\n",
    "        samples = predict_y_samples(dgp.as_prediction_model(), Xs, N_samples).numpy().T\n",
    "\n",
    "    m = np.mean(samples, 0).flatten()\n",
    "    v = np.var(samples, 0).flatten()\n",
    "\n",
    "    ax.plot(Xs.flatten(), m, \"C1\")\n",
    "    for i in [1, 2]:\n",
    "        lower = m - i * np.sqrt(v)\n",
    "        upper = m + i * np.sqrt(v)\n",
    "        ax.fill_between(Xs.flatten(), lower, upper, color=\"C1\", alpha=0.3)\n",
    "    ax.plot(X, Y, \"kx\", alpha=0.5)\n",
    "    ax.set_ylim(Y.min() - Y_MARGIN, Y.max() + Y_MARGIN)\n",
    "    ax.set_xlabel(\"time\")\n",
    "    ax.set_ylabel(\"acceleration\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7576bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_mean_and_var(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2db26",
   "metadata": {},
   "source": [
    "The deep GP model can handle the heteroscedastic noise in the dataset as well as the sharp-ish transition at $0.3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1cf83",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we created a two layer deep Gaussian process with a latent variable base layer to model a heteroscedastic dataset using GPflux.\n",
    "\n",
    "\n",
    "[1] Silverman, B. W. (1985) “Some aspects of the spline smoothing approach to non-parametric curve fitting”. Journal of the Royal Statistical Society series B 47, 1-52."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252fac0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
