{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Conditional Density Estimation\n",
    "\n",
    "In this notebook, we explore the use of Deep Gaussian processes and Latent Variables to model a dataset with heteroscedastic noise."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "import gpflow\n",
    "import gpflux\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "tf.keras.backend.set_floatx(\"float64\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def motorcycle_data():\n",
    "    \"\"\" Return inputs and outputs for the motorcycle dataset. We normalise the outputs. \"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(\"./data/motor.csv\", index_col=0)\n",
    "    X, Y = df[\"times\"].values.reshape(-1, 1), df[\"accel\"].values.reshape(-1, 1)\n",
    "    Y = (Y - Y.mean()) / Y.std()\n",
    "    X /= X.max()\n",
    "    return X, Y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X, Y = motorcycle_data()\n",
    "num_data, d_xim = X.shape\n",
    "\n",
    "X_MARGIN, Y_MARGIN = 0.1, 0.5\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y, marker='x', color='k');\n",
    "ax.set_ylim(Y.min() - Y_MARGIN, Y.max() + Y_MARGIN);\n",
    "ax.set_xlim(X.min() - X_MARGIN, X.max() + X_MARGIN);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standard single layer Sparse Variational GP\n",
    "\n",
    "We first show that a single layer SVGP performs quite poorly on this dataset. In the following code block we define the kernel, inducing variable, GP layer and likelihood of our shallow GP:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NUM_INDUCING = 20\n",
    "\n",
    "kernel = gpflow.kernels.SquaredExponential()\n",
    "inducing_variable = gpflow.inducing_variables.InducingPoints(\n",
    "    np.linspace(X.min(), X.max(), NUM_INDUCING).reshape(-1, 1)\n",
    ")\n",
    "gp_layer = gpflux.layers.GPLayer(\n",
    "    kernel, inducing_variable, num_data=num_data, num_latent_gps=1\n",
    ")\n",
    "likelihood_layer = gpflux.layers.LikelihoodLayer(gpflow.likelihoods.Gaussian(0.1))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now encapsulate `gp_layer` in a GPflux DeepGP model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "single_layer_dgp = gpflux.models.DeepGP([gp_layer], likelihood_layer)\n",
    "model = single_layer_dgp.as_training_model()\n",
    "model.compile(tf.optimizers.Adam(0.01))\n",
    "\n",
    "history = model.fit({\"inputs\": X, \"targets\": Y}, epochs=int(1e3), verbose=0)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(history.history[\"loss\"])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "num_data_test = 200\n",
    "X_test = np.linspace(X.min() - X_MARGIN, X.max() + X_MARGIN, num_data_test).reshape(-1, 1)\n",
    "model = single_layer_dgp.as_prediction_model()\n",
    "out = model(X_test)\n",
    "\n",
    "mu = out.y_mean.numpy().squeeze()\n",
    "var = out.y_var.numpy().squeeze()\n",
    "X_test = X_test.squeeze()\n",
    "\n",
    "for i in [1, 2]:\n",
    "    lower = mu - i * np.sqrt(var)\n",
    "    upper = mu + i * np.sqrt(var)\n",
    "    ax.fill_between(X_test, lower, upper, color=\"C1\", alpha=0.3)\n",
    "\n",
    "ax.set_ylim(Y.min() - Y_MARGIN, Y.max() + Y_MARGIN)\n",
    "ax.set_xlim(X.min() - X_MARGIN, X.max() + X_MARGIN)\n",
    "ax.plot(X, Y, \"kx\", alpha=0.5)\n",
    "ax.plot(X_test, mu, \"C1\")\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('acc')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The errorbars of the single layer model are not good."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Gaussian process with latent variables\n",
    "\n",
    "To tackle the problem we suggest a Deep Gaussian process with a latent variable in the first layer. The latent variable will be able to capture the heteroskedasticity, while the two layered deep GP is able to model the sharp transitions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Latent Variable Layer\n",
    "\n",
    "This layer concatenates the inputs with a latent variable. See Dutordoir, Salimbeni et al. Conditional Density with Gaussian processes (2018) for full details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w_dim = 1\n",
    "prior_means = np.zeros(w_dim)\n",
    "prior_std = np.ones(w_dim)\n",
    "encoder = gpflux.encoders.DirectlyParameterizedNormalDiag(num_data, w_dim)\n",
    "prior = tfp.distributions.MultivariateNormalDiag(prior_means, prior_std)\n",
    "lv = gpflux.layers.LatentVariableLayer(prior, encoder)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First GP layer\n",
    "\n",
    "GP Layer with two dimensional input because it acts on the inputs and the one-dimensional latent variable. We use a Squared Exponential kernel, a zero mean function, and inducing points, whose pseudo input locations are carefully chosen."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "kernel = gpflow.kernels.SquaredExponential(lengthscales=[.05, .2], variance=1.)\n",
    "inducing_variable = gpflow.inducing_variables.InducingPoints(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            np.linspace(X.min(), X.max(), NUM_INDUCING).reshape(-1, 1),\n",
    "            np.random.randn(NUM_INDUCING, 1),\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "gp_layer = gpflux.layers.GPLayer(\n",
    "    kernel,\n",
    "    inducing_variable,\n",
    "    num_data=num_data,\n",
    "    num_latent_gps=1,\n",
    "    mean_function=gpflow.mean_functions.Zero(),\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Second GP layer\n",
    "\n",
    "Final layer GP with Squared Exponential kernel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "kernel = gpflow.kernels.SquaredExponential()\n",
    "inducing_variable = gpflow.inducing_variables.InducingPoints(\n",
    "    np.random.randn(NUM_INDUCING, 1),\n",
    ")\n",
    "gp_layer2 = gpflux.layers.GPLayer(\n",
    "    kernel,\n",
    "    inducing_variable,\n",
    "    num_data=num_data,\n",
    "    num_latent_gps=1,\n",
    "    mean_function=gpflow.mean_functions.Identity(),\n",
    ")\n",
    "gp_layer2.q_sqrt.assign(gp_layer.q_sqrt * 1e-5);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "likelihood_layer = gpflux.layers.LikelihoodLayer(gpflow.likelihoods.Gaussian(0.01))\n",
    "dgp = gpflux.models.DeepGP([lv, gp_layer, gp_layer2], likelihood_layer)\n",
    "gpflow.utilities.print_summary(dgp, fmt=\"notebook\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fit\n",
    "\n",
    "We can now fit the model. Because of the `DirectlyParameterizedEncoder` it is important to set the batch size to the number of datapoints and turn off shuffle. This is so that we use the associated latent variable for each datapoint. If we would use an Amortized Encoder network this would not be necessary."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "model = dgp.as_training_model()\n",
    "model.compile(tf.optimizers.Adam(0.005))\n",
    "history = model.fit({\"inputs\": X, \"targets\": Y}, epochs=int(10e3), verbose=0, batch_size=num_data, shuffle=False)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 10.3615 - latent_variable_layer_local_kl: 9.5638 - gp_layer_2_prior_kl: 0.2760 - gp_layer_4_prior_kl: 0.6132\n",
      "Epoch 294/7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.1598 - latent_variable_layer_local_kl: 9.5588 - gp_layer_2_prior_kl: 0.2765 - gp_layer_4_prior_kl: 0.6131\n",
      "Epoch 295/7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.1892 - latent_variable_layer_local_kl: 9.5538 - gp_layer_2_prior_kl: 0.2770 - gp_layer_4_prior_kl: 0.6129\n",
      "Epoch 296/7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.1331 - latent_variable_layer_local_kl: 9.5487 - gp_layer_2_prior_kl: 0.2775 - gp_layer_4_prior_kl: 0.6128\n",
      "Epoch 297/7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.0920 - latent_variable_layer_local_kl: 9.5437 - gp_layer_2_prior_kl: 0.2780 - gp_layer_4_prior_kl: 0.6127\n",
      "Epoch 298/7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.0540 - latent_variable_layer_local_kl: 9.5386 - gp_layer_2_prior_kl: 0.2784 - gp_layer_4_prior_kl: 0.6125\n",
      "Epoch 299/7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.8470 - latent_variable_layer_local_kl: 9.5336 - gp_layer_2_prior_kl: 0.2787 - gp_layer_4_prior_kl: 0.6124\n",
      "Epoch 300/7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.0443 - latent_variable_layer_local_kl: 9.5286 - gp_layer_2_prior_kl: 0.2790 - gp_layer_4_prior_kl: 0.6122\n",
      "Epoch 301/7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.2227 - latent_variable_layer_local_kl: 9.5236 - gp_layer_2_prior_kl: 0.2792 - gp_layer_4_prior_kl: 0.6121\n",
      "Epoch 302/7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.9484 - latent_variable_layer_local_kl: 9.5185 - gp_layer_2_prior_kl: 0.2796 - gp_layer_4_prior_kl: 0.6119\n",
      "Epoch 303/7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.9349 - latent_variable_layer_local_kl: 9.5135 - gp_layer_2_prior_kl: 0.2799 - gp_layer_4_prior_kl: 0.6118\n",
      "Epoch 304/7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.8659 - latent_variable_layer_local_kl: 9.5086 - gp_layer_2_prior_kl: 0.2802 - gp_layer_4_prior_kl: 0.6117\n",
      "Epoch 305/7000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.9643 - latent_variable_layer_local_kl: 9.5036 - gp_layer_2_prior_kl: 0.2805 - gp_layer_4_prior_kl: 0.6115\n",
      "Epoch 306/7000\n",
      "1/1 [==============================] - ETA: 0s - loss: 9.8475 - latent_variable_layer_local_kl: 9.4986 - gp_layer_2_prior_kl: 0.2807 - gp_layer_4_prior_kl: 0.6114"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('gpflow2': conda)"
  },
  "interpreter": {
   "hash": "f5f45e34fe652d34c37da17ab3d28157e258abad05dd1da4f29284863e79e5cf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}