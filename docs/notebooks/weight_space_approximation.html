
<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Weight Space Approximation with Random Fourier Features &#8212; GPflux 0.1.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=eafc0fe6" />
    <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css" />
    <link rel="stylesheet" type="text/css" href="../_static/pydata-custom.css?v=c27e38e3" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=2389946f"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/weight_space_approximation';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Efficient Posterior Gaussian Process Sampling" href="efficient_posterior_sampling.html" />
    <link rel="prev" title="Efficient sampling with Gaussian processes and Random Fourier Features" href="efficient_sampling.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="GPflux 0.1.0 documentation - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="GPflux 0.1.0 documentation - Home"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../index.html">
                        GPflux
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="benchmarks.html">
                        Benchmarks
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../tutorials.html">
                        Tutorials
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../autoapi/gpflux/index.html">
                        API Reference
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/secondmind-labs/gpflux" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i></span>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item">
<nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../index.html">
                        GPflux
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="benchmarks.html">
                        Benchmarks
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../tutorials.html">
                        Tutorials
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../autoapi/gpflux/index.html">
                        API Reference
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/secondmind-labs/gpflux" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i></span>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Introductory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction to GPflux</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpflux_features.html">Why GPflux is a modern (deep) GP library</a></li>
<li class="toctree-l1"><a class="reference internal" href="deep_cde.html">Deep Gaussian processes with Latent Variables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="deep_gp_samples.html">Deep GP samples</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpflux_with_keras_layers.html">Hybrid Deep GP models: combining GP and Neural Network layers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sampling</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="efficient_sampling.html">Efficient sampling with Gaussian processes and Random Fourier Features</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Weight Space Approximation with Random Fourier Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="efficient_posterior_sampling.html">Efficient Posterior Gaussian Process Sampling</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../tutorials.html" class="nav-link">Tutorials</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Weight...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="Weight-Space-Approximation-with-Random-Fourier-Features">
<h1>Weight Space Approximation with Random Fourier Features<a class="headerlink" href="#Weight-Space-Approximation-with-Random-Fourier-Features" title="Permalink to this heading">#</a></h1>
<p>This notebook demonstrates how to approximate an exact Gaussian process regression model (GPR) with random Fourier features in weight space. The end result is Figure 1 from from Wilson et al. “Efficiently sampling functions from Gaussian process posteriors” <span id="id1">[<a class="reference internal" href="../index.html#id13" title="James Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc Deisenroth. Efficiently sampling functions from Gaussian process posteriors. In International Conference on Machine Learning. 2020.">WBT+20</a>]</span>. This figure demonstrates that approximating an exact GPR model in weight space becomes increasingly more difficult as the training data grows. While the approximation remains accurate in areas within the
training data and far away from the training data, predictions close to the training data points (but outside the training data interval) become less reliable. Note that Wilson et al. provide a method to alleviate exactly this problem; however, this is outside the scope of this notebook, where the emphasis is on how to build a weight space approximated Gaussian process model with random Fourier features in <code class="docutils literal notranslate"><span class="pre">gpflux</span></code>.</p>
<p>The basic idea is to approximate a stationary kernel <span class="math notranslate nohighlight">\(k(X,X^\prime)\)</span> for one-dimensional inputs <span class="math notranslate nohighlight">\(X \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(X^\prime \in \mathbb{R}\)</span> according to Bochner’s theorem:</p>
<div class="math notranslate nohighlight">
\[k(X, X^\prime) \approx \sum_{i=1}^I \phi_i(X) \phi_i(X^\prime),\]</div>
<p>with <span class="math notranslate nohighlight">\(I\)</span> Fourier features <span class="math notranslate nohighlight">\(\phi_i\)</span> following Rahimi and Recht “Random features for large-scale kernel machines” (NeurIPS, 2007) defined as</p>
<div class="math notranslate nohighlight">
\[\phi_i(X) = \sqrt{\frac{2 \sigma^2}{l}} \cos(\theta_i X + \tau_i),\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma^2\)</span> refers to the kernel variance and <span class="math notranslate nohighlight">\(l\)</span> to the kernel lengthscale. <span class="math notranslate nohighlight">\(\theta_i\)</span> and <span class="math notranslate nohighlight">\(\tau_i\)</span> are randomly drawn hyperparameters that determine each feature function <span class="math notranslate nohighlight">\(\phi_i\)</span>. The hyperparameter <span class="math notranslate nohighlight">\(\theta_i\)</span> is randomly drawn from the kernel’s spectral density. The spectral density of a stationary kernel is obtained by interpreting the kernel as a function of one argument only (i.e. the distance between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(X^\prime\)</span>) and performing
a Fourier transform on that function, resulting in an unnormalised probability density (from which samples can be obtained). The hyperparameter <span class="math notranslate nohighlight">\(\tau_i\)</span> is obtained by sampling from a uniform distribution <span class="math notranslate nohighlight">\(\tau_i \sim \mathcal{U}(0,2\pi)\)</span>. Note that both <span class="math notranslate nohighlight">\(\theta_i\)</span> and <span class="math notranslate nohighlight">\(\tau_i\)</span> are fixed and not optimised over. An interesting direction of future research is how to automatically identify those (but this is outside the scope of this notebook). If we drew infinitely many
samples, i.e. <span class="math notranslate nohighlight">\(I \rightarrow \infty\)</span>, we would recover the true kernel perfectly.</p>
<p>The kernel approximation specified above enables you to express a supervised inference problem with training data <span class="math notranslate nohighlight">\(\mathcal{D} = \{(X_n,y_n)\}_{n=1,...,N}\)</span> in weight space view as</p>
<div class="math notranslate nohighlight">
\[p(\textbf{w} | \mathcal{D}) = \frac{\prod_{n=1}^N p(y_n| \textbf{w}^\intercal \boldsymbol{\phi}(X_n), \sigma_\epsilon^2) p(\textbf{w})}{p(\mathcal{D})},\]</div>
<p>where we assume <span class="math notranslate nohighlight">\(p(\textbf{w})\)</span> to be a standard normal multivariate prior and <span class="math notranslate nohighlight">\(p(y_n| \textbf{w}^\intercal \boldsymbol{\phi}(X_n), \sigma_\epsilon^2)\)</span> to be a univariate Gaussian observation model of the i.i.d. likelihood with mean <span class="math notranslate nohighlight">\(\textbf{w}^\intercal \boldsymbol{\phi}(X_n)\)</span> and noise variance <span class="math notranslate nohighlight">\(\sigma_\epsilon^2\)</span>. The boldface notation <span class="math notranslate nohighlight">\(\boldsymbol{\phi}(X_n)\)</span> refers to the vector-valued feature function that evaluates all features from <span class="math notranslate nohighlight">\(1\)</span> up to <span class="math notranslate nohighlight">\(I\)</span>
for one particular input <span class="math notranslate nohighlight">\(X_n\)</span>. Under these assumptions, the posterior <span class="math notranslate nohighlight">\(p(\textbf{w} | \mathcal{D})\)</span> enjoys a closed form and is Gaussian. Predictions can readily be obtained by sampling <span class="math notranslate nohighlight">\(\textbf{w}\)</span> and evaluating the function sample <span class="math notranslate nohighlight">\(\textbf{w}\)</span> at new locations <span class="math notranslate nohighlight">\(\{X_{n^\star}^\star\}_{n^\star=1,...,N^\star}\)</span> as <span class="math notranslate nohighlight">\(\{\textbf{w}^\intercal \boldsymbol{\phi}(X_{n^\star}^\star)\}_{n^\star=1,...,N^\star}\)</span>.</p>
<p>The advantage of expressing a Gaussian process in weight space is that functions are represented as weight vectors <span class="math notranslate nohighlight">\(\textbf{w}\)</span> (rather than actual functions <span class="math notranslate nohighlight">\(f(\cdot)\)</span>) from which samples can be obtained a priori without knowing where the function should be evaluated. When expressing a Gaussian process in function space view the latter is not possible, i.e. a function <span class="math notranslate nohighlight">\(f(\cdot)\)</span> cannot be sampled without knowing where to evaluate the function, namely at
<span class="math notranslate nohighlight">\(\{X_{n^\star}^\star\}_{n^\star=1,...,N^\star}\)</span>. Weight space approximated Gaussian processes therefore hold the potential to sample efficiently from Gaussian process posteriors, which is desirable in vanilla supervised learning but also in domains such as Bayesian optimisation or model-based reinforcement learning.</p>
<p>In the following example, we compare a weight space approximated GPR model (WSA model) with both a proper GPR model and a sparse variational Gaussian Process model (SVGP). GPR models and SVGP models are implemented in <code class="docutils literal notranslate"><span class="pre">gpflow</span></code>, but the two necessary ingredients for building the WSA model are part of <code class="docutils literal notranslate"><span class="pre">gpflux</span></code>: these are random Fourier feature functions via the <code class="docutils literal notranslate"><span class="pre">RandomFourierFeaturesCosine</span></code> class, and approximate kernels based on Bochner’s theorem (or any other theorem that approximates a
kernel with a finite number of feature functions, e.g. Mercer) via the <code class="docutils literal notranslate"><span class="pre">KernelWithFeatureDecomposition</span></code> class.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;font.size&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">})</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">import</span> <span class="nn">gpflow</span> <span class="k">as</span> <span class="nn">gpf</span>
<span class="kn">from</span> <span class="nn">gpflow.config</span> <span class="kn">import</span> <span class="n">default_float</span>
<span class="kn">from</span> <span class="nn">gpflow.models</span> <span class="kn">import</span> <span class="n">GPR</span><span class="p">,</span> <span class="n">SVGP</span>
<span class="kn">from</span> <span class="nn">gpflow.kernels</span> <span class="kn">import</span> <span class="n">RBF</span><span class="p">,</span> <span class="n">Matern52</span>
<span class="kn">from</span> <span class="nn">gpflow.likelihoods</span> <span class="kn">import</span> <span class="n">Gaussian</span>
<span class="kn">from</span> <span class="nn">gpflow.inducing_variables</span> <span class="kn">import</span> <span class="n">InducingPoints</span>

<span class="kn">from</span> <span class="nn">gpflux.layers.basis_functions.fourier_features</span> <span class="kn">import</span> <span class="n">RandomFourierFeaturesCosine</span>
<span class="kn">from</span> <span class="nn">gpflux.sampling.kernel_with_feature_decomposition</span> <span class="kn">import</span> <span class="n">KernelWithFeatureDecomposition</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2024-06-20 12:04:24.060027: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-20 12:04:24.092095: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-20 12:04:24.092795: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-20 12:04:24.834054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
</pre></div></div>
</div>
<p>Our aim is to demonstrate the decrease in predictive quality of a WSA model when increasing the number of training points. To that end, we perform two sets of experiments: one with few and one with many training data points. Each experiment compares a WSA model to an exact GPR and to an approximate SVGP model, resulting in six plots all in all.</p>
<p>We first define settings that remain the same across the two sets of experiments, like the interval of the training points, aspects of the generative model (i.e. kernel variance and lengthscale, and the variance of the observation model), and the number of feature functions of the WSA model.</p>
<p>The only aspect that is different across both experimental settings is the number of training data points. We increase the number of inducing points for the SVGP model to cope with this.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># experiment parameters that are the same for both sets of experiments</span>
<span class="n">X_interval</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.14</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>  <span class="c1"># interval where training points live</span>
<span class="n">lengthscale</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># lengthscale for the kernel (which is not learned in all experiments, the kernel variance is 1)</span>
<span class="n">number_of_features</span> <span class="o">=</span> <span class="mi">2000</span>  <span class="c1"># number of basis functions for weight-space approximated kernels</span>
<span class="n">noise_variance</span> <span class="o">=</span> <span class="mf">1e-3</span>  <span class="c1"># noise variance of the likelihood (which is not learned in all experiments)</span>
<span class="n">number_of_test_samples</span> <span class="o">=</span> <span class="mi">1024</span>  <span class="c1"># number of evaluation points for prediction</span>
<span class="n">number_of_function_samples</span> <span class="o">=</span> <span class="p">(</span>
    <span class="mi">20</span>  <span class="c1"># number of function samples to be drawn from (approximate) posteriors</span>
<span class="p">)</span>

<span class="c1"># experiment parameters that differ across both sets of experiments</span>
<span class="n">number_of_train_samples</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>  <span class="c1"># number of training points</span>
<span class="n">number_of_inducing_points</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>  <span class="c1"># number of inducing points for SVGP models</span>

<span class="c1"># kernel class</span>
<span class="n">kernel_class</span> <span class="o">=</span> <span class="n">Matern52</span>  <span class="c1"># set alternatively kernel_class = RBF</span>

<span class="c1"># plotting configuration</span>
<span class="n">x_lim</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">y_lim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>We proceed by generating the training data for both experimental settings from a ground truth function which is a sample from a prior zero-mean GP with a predefined kernel (in our case, we use a <code class="docutils literal notranslate"><span class="pre">Matern52</span></code> kernel but we could have chosen an <code class="docutils literal notranslate"><span class="pre">RBF</span></code> kernel – both of which are defined in <code class="docutils literal notranslate"><span class="pre">gpflow</span></code>).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate training data and evaluation points for both sets of experiments</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel_class</span><span class="p">(</span><span class="n">lengthscales</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">)</span>  <span class="c1"># kernel object to draw training dataset from</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X_star</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>  <span class="c1"># training points, training observations, and test points for evaluation</span>

<span class="c1"># 1st iteration: experiments with few training points -- 2nd iteration: experiments with many training points</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">number_of_train_samples</span><span class="p">)):</span>

    <span class="c1"># training points</span>
    <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">X_interval</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">stop</span><span class="o">=</span><span class="n">X_interval</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">num</span><span class="o">=</span><span class="n">number_of_train_samples</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

    <span class="c1"># training observations generated from a zero-mean GP corrupted with Gaussian noise</span>
    <span class="n">kXX</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
    <span class="n">kXX_plus_noise_var</span> <span class="o">=</span> <span class="n">kXX</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">kXX</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">kXX</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise_variance</span>
    <span class="n">lXX</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">kXX_plus_noise_var</span><span class="p">)</span>
    <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">lXX</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">([</span><span class="n">number_of_train_samples</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="c1"># test points for evaluation</span>
    <span class="n">X_star</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">x_lim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">stop</span><span class="o">=</span><span class="n">x_lim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">num</span><span class="o">=</span><span class="n">number_of_test_samples</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">for</span></code> loop below iterates through both experimental settings with few and many training examples respectively. In each iteration, the GPR model is built first (and its prediction is used as “ground truth” to compare with the remaining models) followed by the SVGP model (which requires optimisation to identify internal parameters) and the WSA model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create subplot frame</span>
<span class="c1"># 1st row: experiments with few training examples, 2nd row: experiments with many training examples</span>
<span class="c1"># 1st col: exact Gaussian process regression (GPR), 2nd col: sparse variational Gaussian process model (SVGP),</span>
<span class="c1"># 3rd col: weight space approximation (WSA) of the exact GPR posterior with random Fourier features</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>


<span class="c1"># 1st iteration: experiments with few training points -- 2nd iteration: experiments with many training points</span>
<span class="k">for</span> <span class="n">experiment</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">number_of_train_samples</span><span class="p">)):</span>

    <span class="c1"># subplot titles and axis labels</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Exact GP $N=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">number_of_train_samples</span><span class="p">[</span><span class="n">experiment</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot;$&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Sparse GP $N=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">number_of_train_samples</span><span class="p">[</span><span class="n">experiment</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot;$&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
        <span class="s2">&quot;Weight Space GP $N=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">number_of_train_samples</span><span class="p">[</span><span class="n">experiment</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot;$&quot;</span>
    <span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$f(X)$&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">experiment</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$X$&quot;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$X$&quot;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$X$&quot;</span><span class="p">)</span>

    <span class="c1"># plot training point locations X and set axis limits</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>  <span class="c1"># iterate through all three subplots (GPR, SVGP and WSA)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">experiment</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">):</span>  <span class="c1"># as vertical lines for the first set of experiments with few training samples</span>
            <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span>
                <span class="n">X</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span> <span class="n">ymin</span><span class="o">=</span><span class="n">y_lim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ymax</span><span class="o">=</span><span class="n">y_lim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;lightgrey&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># as fill plots for the second set of experiments with many training samples</span>
            <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
                <span class="n">X</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span> <span class="n">y_lim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_lim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
            <span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">x_lim</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">y_lim</span><span class="p">)</span>

    <span class="c1"># create the GPR &quot;ground truth&quot; model</span>
    <span class="n">gpr_model</span> <span class="o">=</span> <span class="n">GPR</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">experiment</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">experiment</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">kernel_class</span><span class="p">(</span><span class="n">lengthscales</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">),</span>
        <span class="n">noise_variance</span><span class="o">=</span><span class="n">noise_variance</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># predict function mean and variance, and draw function samples (without observation noise)</span>
    <span class="n">f_mean</span><span class="p">,</span> <span class="n">f_var</span> <span class="o">=</span> <span class="n">gpr_model</span><span class="o">.</span><span class="n">predict_f</span><span class="p">(</span><span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
    <span class="n">f_samples</span> <span class="o">=</span> <span class="n">gpr_model</span><span class="o">.</span><span class="n">predict_f_samples</span><span class="p">(</span>
        <span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">number_of_function_samples</span>
    <span class="p">)</span>
    <span class="n">f_mean_plus_2std</span> <span class="o">=</span> <span class="n">f_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">f_var</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="n">f_mean_minus_2std</span> <span class="o">=</span> <span class="n">f_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">f_var</span> <span class="o">**</span> <span class="mf">0.5</span>

    <span class="c1"># plot mean and std lines from the GPR model as &quot;ground truth&quot; in all three plots</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span> <span class="n">f_mean</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span> <span class="n">f_mean_minus_2std</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span>
        <span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span> <span class="n">f_mean_plus_2std</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span>
        <span class="p">)</span>

    <span class="c1"># visualise GPR model predictions (mean +/- 2 * std and function samples) in the first column</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span>
        <span class="n">f_mean_minus_2std</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">f_mean_plus_2std</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">f_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span> <span class="n">f_samples</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.2</span>
        <span class="p">)</span>

    <span class="c1"># create the SVGP model</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">experiment</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="p">):</span>  <span class="c1"># inducing points equal the training data for the first experiment with few training points</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">experiment</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># inducing points are randomly chosen for the second experiment with many training points</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X_interval</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_interval</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">number_of_inducing_points</span><span class="p">[</span><span class="n">experiment</span><span class="p">])[</span>
            <span class="o">...</span><span class="p">,</span> <span class="kc">None</span>
        <span class="p">]</span>
    <span class="n">svgp_model</span> <span class="o">=</span> <span class="n">SVGP</span><span class="p">(</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">kernel_class</span><span class="p">(</span><span class="n">lengthscales</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">),</span>
        <span class="n">likelihood</span><span class="o">=</span><span class="n">Gaussian</span><span class="p">(</span><span class="n">variance</span><span class="o">=</span><span class="n">noise_variance</span><span class="p">),</span>
        <span class="n">inducing_variable</span><span class="o">=</span><span class="n">InducingPoints</span><span class="p">(</span><span class="n">Z</span><span class="o">=</span><span class="n">Z</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">gpf</span><span class="o">.</span><span class="n">set_trainable</span><span class="p">(</span>
        <span class="n">svgp_model</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="kc">False</span>
    <span class="p">)</span>  <span class="c1"># the training data has been sampled from a known kernel!</span>
    <span class="n">gpf</span><span class="o">.</span><span class="n">set_trainable</span><span class="p">(</span><span class="n">svgp_model</span><span class="o">.</span><span class="n">likelihood</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>  <span class="c1"># the likelihood variance is known!</span>
    <span class="n">gpf</span><span class="o">.</span><span class="n">set_trainable</span><span class="p">(</span><span class="n">svgp_model</span><span class="o">.</span><span class="n">inducing_variable</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>  <span class="c1"># inducing point locations are fixed!</span>

    <span class="k">def</span> <span class="nf">optimize_model_with_scipy</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">gpf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Scipy</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">training_loss_closure</span><span class="p">((</span><span class="n">X</span><span class="p">[</span><span class="n">experiment</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">experiment</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])),</span>
            <span class="n">variables</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="s2">&quot;l-bfgs-b&quot;</span><span class="p">,</span>
            <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;disp&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;maxiter&quot;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">},</span>
        <span class="p">)</span>

    <span class="n">optimize_model_with_scipy</span><span class="p">(</span><span class="n">svgp_model</span><span class="p">)</span>

    <span class="c1"># predict function mean and variance, and draw function samples (without observation noise)</span>
    <span class="n">f_mean</span><span class="p">,</span> <span class="n">f_var</span> <span class="o">=</span> <span class="n">svgp_model</span><span class="o">.</span><span class="n">predict_f</span><span class="p">(</span><span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
    <span class="n">f_samples</span> <span class="o">=</span> <span class="n">svgp_model</span><span class="o">.</span><span class="n">predict_f_samples</span><span class="p">(</span>
        <span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">number_of_function_samples</span>
    <span class="p">)</span>
    <span class="n">f_mean_plus_2std</span> <span class="o">=</span> <span class="n">f_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">f_var</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="n">f_mean_minus_2std</span> <span class="o">=</span> <span class="n">f_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">f_var</span> <span class="o">**</span> <span class="mf">0.5</span>

    <span class="c1"># visualise SVGP model predictions (mean +/- 2 * std and function samples) in the second column</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span>
        <span class="n">f_mean_minus_2std</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">f_mean_plus_2std</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">color</span><span class="o">=</span><span class="s2">&quot;purple&quot;</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">f_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span> <span class="n">f_samples</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;purple&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.2</span>
        <span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span> <span class="n">f_mean</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;purple&quot;</span><span class="p">)</span>

    <span class="c1"># visualise predictions at inducing point locations (without observation noise)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">svgp_model</span><span class="o">.</span><span class="n">inducing_variable</span><span class="o">.</span><span class="n">Z</span>
    <span class="n">q_mu</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">svgp_model</span><span class="o">.</span><span class="n">predict_f</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">Z</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">q_mu</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">mfc</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;purple&quot;</span>
    <span class="p">)</span>

    <span class="c1"># create exact GPR model with weight-space approximated kernel (WSA model)</span>
    <span class="n">feature_functions</span> <span class="o">=</span> <span class="n">RandomFourierFeaturesCosine</span><span class="p">(</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">kernel_class</span><span class="p">(</span><span class="n">lengthscales</span><span class="o">=</span><span class="n">lengthscale</span><span class="p">),</span>
        <span class="n">n_components</span><span class="o">=</span><span class="n">number_of_features</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">default_float</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="n">feature_coefficients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">number_of_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">default_float</span><span class="p">())</span>
    <span class="n">kernel</span> <span class="o">=</span> <span class="n">KernelWithFeatureDecomposition</span><span class="p">(</span>
        <span class="n">kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feature_functions</span><span class="o">=</span><span class="n">feature_functions</span><span class="p">,</span> <span class="n">feature_coefficients</span><span class="o">=</span><span class="n">feature_coefficients</span>
    <span class="p">)</span>
    <span class="n">gpr_model</span> <span class="o">=</span> <span class="n">GPR</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">experiment</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">experiment</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span>
        <span class="n">noise_variance</span><span class="o">=</span><span class="n">noise_variance</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># predict function mean and variance, and draw function samples (without observation noise)</span>
    <span class="n">f_mean</span><span class="p">,</span> <span class="n">f_var</span> <span class="o">=</span> <span class="n">gpr_model</span><span class="o">.</span><span class="n">predict_f</span><span class="p">(</span><span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
    <span class="n">f_samples</span> <span class="o">=</span> <span class="n">gpr_model</span><span class="o">.</span><span class="n">predict_f_samples</span><span class="p">(</span>
        <span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">number_of_function_samples</span>
    <span class="p">)</span>
    <span class="n">f_mean_plus_2std</span> <span class="o">=</span> <span class="n">f_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">f_var</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="n">f_mean_minus_2std</span> <span class="o">=</span> <span class="n">f_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">f_var</span> <span class="o">**</span> <span class="mf">0.5</span>

    <span class="c1"># visualise WSA model predictions (mean +/- 2 * std and function samples) in the third column</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span>
        <span class="n">f_mean_minus_2std</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">f_mean_plus_2std</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">f_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span> <span class="n">f_samples</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.2</span>
        <span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">experiment</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_star</span><span class="p">[</span><span class="n">experiment</span><span class="p">],</span> <span class="n">f_mean</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">)</span>


<span class="c1"># show the plot</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_weight_space_approximation_8_0.png" src="../_images/notebooks_weight_space_approximation_8_0.png" />
</div>
</div>
<p>The results are visualised in a 2 <span class="math notranslate nohighlight">\(\times\)</span> 3 plot with 6 subplots. The first row refers to experiments with few training data points and the second row to experiments with many training data points. The first column depicts the exact GPR model in green, the second column the SVGP model in purple and the third column the WSA model in orange. In each plot, training data points are marked in grey (as vertical bars in the first row and fill plots in the second row). We also assume the GPR
model’s prediction as ground truth, which is therefore plotted in all plots as black dashed lines (indicating mean +/- 2 * std).</p>
<p>In each plot, the model’s prediction in terms of mean +/- 2 * std is plotted through fill plots, and function samples from the (approximate) posterior through thin solid lines (thick solid lines depict mean functions in the second and third column). Note that the coloured purple circles in the second column refer to predictions at the inducing point locations of the SVGP model.</p>
<p>It can be seen that, as training data points increase from the first to the second row, the predictions of the WSA model decrease drastically in areas relevant to extrapolation (i.e. close to but not inside the training data interval) because a lot of Fourier features would be required to accurately approximate a function sample drawn from a Matern kernel (because of its non-smooth nature). The same effect would be less severe for a function sample drawn from an RBF kernel that is smoother than
a Matern kernel (and can hence be reliably approximated with fewer Fourier features). Note that the experiment is stochastic because the ground truth function sample from the prior kernel is random. There might be outcomes of the experiment in which the explained effect is less prominent than in other random outcomes – so the last code block might require execution more than once to obtain a clear result.</p>
</section>


                </article>
              
              
              
              
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright Copyright 2021 The GPflux Contributors

Licensed under the Apache License, Version 2.0
.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.1.2.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.14.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>