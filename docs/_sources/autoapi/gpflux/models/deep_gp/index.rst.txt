gpflux.models.deep_gp
=====================

.. py:module:: gpflux.models.deep_gp

.. autoapi-nested-parse::

   This module provides the base implementation for DeepGP models. 





Module Contents
---------------

.. py:class:: LayerWithObservations

   Bases: :py:obj:`gpflux.layers.trackable_layer.TrackableLayer`


   By inheriting from this class, Layers indicate that their :meth:`call`
   method takes a second *observations* argument after the customary
   *layer_inputs* argument.

   This is used to distinguish which layers (unlike most standard Keras
   layers) require the original inputs and/or targets during training.
   For example, it is used by the amortized variational inference in the
   :class:`LatentVariableLayer`.


   .. py:method:: call(layer_inputs: gpflow.base.TensorType, observations: Optional[gpflux.types.ObservationType] = None, training: Optional[bool] = None) -> tensorflow.Tensor
      :abstractmethod:


      The :meth:`call` method of `LayerWithObservations` subclasses should
      accept a second argument, *observations*. In training mode, this will
      be the ``[inputs, targets]`` of the training points; otherwise, it is `None`.



.. py:class:: LikelihoodLayer(likelihood: gpflow.likelihoods.Likelihood)

   Bases: :py:obj:`gpflux.layers.trackable_layer.TrackableLayer`


   A Keras layer that wraps a GPflow :class:`~gpflow.likelihoods.Likelihood`. This layer expects a
   `tfp.distributions.MultivariateNormalDiag` as its input, describing ``q(f)``.
   When training, calling this class computes the negative variational expectation
   :math:`-\mathbb{E}_{q(f)}[\log p(y|f)]` and adds it as a layer loss.
   When not training, it computes the mean and variance of ``y`` under ``q(f)``
   using :meth:`~gpflow.likelihoods.Likelihood.predict_mean_and_var`.

   .. note::

       Use **either** this `LikelihoodLayer` (together with
       `gpflux.models.DeepGP`) **or** `LikelihoodLoss` (e.g. together with a
       `tf.keras.Sequential` model). Do **not** use both at once because
       this would add the loss twice.


   .. py:method:: call(inputs: tensorflow_probability.distributions.MultivariateNormalDiag, targets: Optional[gpflow.base.TensorType] = None, training: bool = None) -> LikelihoodOutputs

      When training (``training=True``), this method computes variational expectations
      (data-fit loss) and adds this information as a layer loss.
      When testing (the default), it computes the posterior mean and variance of ``y``.

      :param inputs: The output distribution of the previous layer. This is currently
          expected to be a :class:`~tfp.distributions.MultivariateNormalDiag`;
          that is, the preceding :class:`~gpflux.layers.GPLayer` should have
          ``full_cov=full_output_cov=False``.
      :returns: a `LikelihoodOutputs` tuple with the mean and variance of ``f`` and,
          if not training, the mean and variance of ``y``.

      .. todo:: Turn this layer into a
          :class:`~tfp.layers.DistributionLambda` as well and return the
          correct :class:`~tfp.distributions.Distribution` instead of a tuple
          containing mean and variance only.



.. py:class:: Sample

   Bases: :py:obj:`abc.ABC`


   This class represents a sample from a GP that you can evaluate by using the ``__call__``
   at new locations within the support of the GP.

   Importantly, the same function draw (sample) is evaluated when calling it multiple
   times. This property is called consistency. Achieving consistency for vanilla GPs is costly
   because it scales cubically with the number of evaluation points,
   but works with any kernel. It is implemented in
   :meth:`_efficient_sample_conditional_gaussian`.
   For :class:`KernelWithFeatureDecomposition`, the more efficient approach
   following :cite:t:`wilson2020efficiently` is implemented in
   :meth:`_efficient_sample_matheron_rule`.

   See the tutorial notebooks `Efficient sampling
   <../../../../notebooks/efficient_sampling.ipynb>`_ and `Weight Space
   Approximation with Random Fourier Features
   <../../../../notebooks/weight_space_approximation.ipynb>`_ for an
   in-depth overview.


   .. py:method:: __call__(X: gpflow.base.TensorType) -> tensorflow.Tensor
      :abstractmethod:


      Return the evaluation of the GP sample :math:`f(X)` for :math:`f \sim GP(0, k)`.

      :param X: The inputs, a tensor with the shape ``[N, D]``, where ``D`` is the
          input dimensionality.
      :return: Function values, a tensor with the shape ``[N, P]``, where ``P`` is the
          output dimensionality.



   .. py:method:: __add__(other: Union[Sample, Callable[[gpflow.base.TensorType], gpflow.base.TensorType]]) -> Sample

      Allow for the summation of two instances that implement the ``__call__`` method.



.. py:class:: DeepGP(f_layers: List[gpflow.keras.tf_keras.layers.Layer], likelihood: Union[gpflux.layers.LikelihoodLayer, gpflow.likelihoods.Likelihood], *, input_dim: Optional[int] = None, target_dim: Optional[int] = None, default_model_class: Type[gpflow.keras.tf_keras.Model] = tf_keras.Model, num_data: Optional[int] = None)

   Bases: :py:obj:`gpflow.base.Module`


   This class combines a sequential function model ``f(x) = fₙ(⋯ (f₂(f₁(x))))``
   and a likelihood ``p(y|f)``.

   Layers might depend on both inputs x and targets y during training by
   inheriting from :class:`~gpflux.layers.LayerWithObservations`; those will
   be passed the argument ``observations=[inputs, targets]``.

   When data is used with methods in this class (e.g. :meth:`predict_f` method), it needs to
   be with ``dtype`` corresponding to GPflow's default dtype as in :meth:`~gpflow.default_float()`.

   .. note:: This class is **not** a `tf.keras.Model` subclass itself. To access
      Keras features, call either :meth:`as_training_model` or :meth:`as_prediction_model`
      (depending on the use-case) to create a `tf.keras.Model` instance. See the method docstrings
      for more details.

   :param f_layers: The layers ``[f₁, f₂, …, fₙ]`` describing the latent
       function ``f(x) = fₙ(⋯ (f₂(f₁(x))))``.
   :param likelihood: The layer for the likelihood ``p(y|f)``. If this is a
       GPflow likelihood, it will be wrapped in a :class:`~gpflux.layers.LikelihoodLayer`.
       Alternatively, you can provide a :class:`~gpflux.layers.LikelihoodLayer` explicitly.
   :param input_dim: The input dimensionality.
   :param target_dim: The target dimensionality.
   :param default_model_class: The default for the *model_class* argument of
       :meth:`as_training_model` and :meth:`as_prediction_model`;
       see the :attr:`default_model_class` attribute.
   :param num_data: The number of points in the training dataset; see the
       :attr:`num_data` attribute.
       If you do not specify a value for this parameter explicitly, it is automatically
       detected from the :attr:`~gpflux.layers.GPLayer.num_data` attribute in the GP layers.


   .. py:attribute:: f_layers
      :type:  List[gpflow.keras.tf_keras.layers.Layer]

      A list of all layers in this DeepGP (just :attr:`likelihood_layer` is separate). 



   .. py:attribute:: likelihood_layer
      :type:  gpflux.layers.LikelihoodLayer

      The likelihood layer. 



   .. py:attribute:: default_model_class
      :type:  Type[gpflow.keras.tf_keras.Model]

      The default for the *model_class* argument of :meth:`as_training_model` and
      :meth:`as_prediction_model`. This must have the same semantics as `tf.keras.Model`,
      that is, it must accept a list of inputs and an output. This could be
      `tf.keras.Model` itself or `gpflux.optimization.NatGradModel` (but not, for
      example, `tf.keras.Sequential`).



   .. py:attribute:: num_data
      :type:  int

      The number of points in the training dataset. This information is used to
      obtain correct scaling between the data-fit and the KL term in the evidence
      lower bound (:meth:`elbo`).



   .. py:method:: _validate_num_data(f_layers: List[gpflow.keras.tf_keras.layers.Layer], num_data: Optional[int] = None) -> int
      :staticmethod:


      Check that the :attr:`~gpflux.layers.gp_layer.GPLayer.num_data`
      attributes of all layers in *f_layers* are consistent with each other
      and with the (optional) *num_data* argument.

      :returns: The validated number of datapoints.



   .. py:method:: _validate_dtype(x: gpflow.base.TensorType) -> None
      :staticmethod:


      Check that data ``x`` is of correct ``dtype``, corresponding to GPflow's default dtype as
      defined by :meth:`~gpflow.default_float()`.

      :raise ValueError: If ``x`` is of incorrect ``dtype``.



   .. py:method:: _evaluate_deep_gp(inputs: gpflow.base.TensorType, targets: Optional[gpflow.base.TensorType], training: Optional[bool] = None) -> tensorflow.Tensor

      Evaluate ``f(x) = fₙ(⋯ (f₂(f₁(x))))`` on the *inputs* argument.

      Layers that inherit from :class:`~gpflux.layers.LayerWithObservations`
      are passed the additional keyword argument ``observations=[inputs,
      targets]`` if *targets* contains a value, or ``observations=None`` when
      *targets* is `None`.



   .. py:method:: _evaluate_likelihood(f_outputs: gpflow.base.TensorType, targets: Optional[gpflow.base.TensorType], training: Optional[bool] = None) -> tensorflow.Tensor

      Call the `likelihood_layer` on *f_outputs*, which adds the
      corresponding layer loss when training.



   .. py:method:: predict_f(inputs: gpflow.base.TensorType) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

      :returns: The mean and variance (not the scale!) of ``f``, for compatibility with GPflow
         models.
      :raise ValueError: If ``x`` is of incorrect ``dtype``.

      .. note:: This method does **not** support ``full_cov`` or ``full_output_cov``.



   .. py:method:: elbo(data: Tuple[gpflow.base.TensorType, gpflow.base.TensorType]) -> tensorflow.Tensor

      :returns: The ELBO (not the per-datapoint loss!), for compatibility with GPflow models.



   .. py:method:: as_training_model(model_class: Optional[Type[gpflow.keras.tf_keras.Model]] = None) -> gpflow.keras.tf_keras.Model

      Construct a `tf.keras.Model` instance that requires you to provide both ``inputs``
      and ``targets`` to its call. This information is required for
      training the model, because the ``targets`` need to be passed to the `likelihood_layer` (and
      to :class:`~gpflux.layers.LayerWithObservations` instances such as
      :class:`~gpflux.layers.LatentVariableLayer`\ s, if present).

      When compiling the returned model, do **not** provide any additional
      losses (this is handled by the :attr:`likelihood_layer`).

      Train with

      .. code-block:: python

          model.compile(optimizer)  # do NOT pass a loss here
          model.fit({"inputs": X, "targets": Y}, ...)

      See `Keras's Endpoint layer pattern
      <https://keras.io/examples/keras_recipes/endpoint_layer_pattern/>`_
      for more details.

      .. note:: Use `as_prediction_model` if you want only to predict, and do not want to pass in
         a dummy array for the targets.

      :param model_class: The model class to use; overrides `default_model_class`.



   .. py:method:: as_prediction_model(model_class: Optional[Type[gpflow.keras.tf_keras.Model]] = None) -> gpflow.keras.tf_keras.Model

      Construct a `tf.keras.Model` instance that requires only ``inputs``,
      which means you do not have to provide dummy target values when
      predicting at test points.

      Predict with

      .. code-block:: python

          model.predict(Xtest, ...)

      .. note:: The returned model will not support training; for that, use `as_training_model`.

      :param model_class: The model class to use; overrides `default_model_class`.



