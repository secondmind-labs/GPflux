gpflux.optimization
===================

.. py:module:: gpflux.optimization

.. autoapi-nested-parse::

   Optimization-related modules, currently just contains the `NatGradModel`
   and `NatGradWrapper` classes to integrate
   `gpflow.optimizers.NaturalGradient` with Keras.



Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/gpflux/optimization/keras_natgrad/index




Package Contents
----------------

.. py:class:: NatGradModel

   Bases: :py:obj:`gpflow.keras.tf_keras.Model`


   This is a drop-in replacement for `tf.keras.Model` when constructing GPflux
   models using the functional Keras style, to make it work with the
   NaturalGradient optimizers for q(u) distributions in GP layers.

   You must set the `natgrad_layers` property before compiling the model. Set it
   to the list of all :class:`~gpflux.layers.GPLayer`\ s you want to train using
   natural gradients. You can also set it to `True` to include all of them.

   This model's :meth:`compile` method has to be passed a list of optimizers, which
   must be one `gpflow.optimizers.NaturalGradient` instance per natgrad-trained
   :class:`~gpflux.layers.GPLayer`, followed by a regular optimizer (e.g.
   `tf.keras.optimizers.Adam`) as the last element to handle all other
   parameters (hyperparameters, inducing point locations).


   .. py:property:: natgrad_layers
      :type: List[gpflux.layers.gp_layer.GPLayer]

      The list of layers in this model that should be optimized using
      `~gpflow.optimizers.NaturalGradient`.

      :getter: Returns a list of the layers that should be trained using
          `~gpflow.optimizers.NaturalGradient`.
      :setter: Sets the layers that should be trained using
          `~gpflow.optimizers.NaturalGradient`. Can be an explicit list or a `bool`:
          If set to `True`, it will select all `GPLayer` instances in the model layers.



   .. py:property:: optimizer
      :type: gpflow.keras.tf_keras.optimizers.Optimizer

      HACK to cope with Keras's callbacks such as
      :class:`~tf.keras.callbacks.ReduceLROnPlateau`
      and
      :class:`~tf.keras.callbacks.LearningRateScheduler`
      having been hardcoded for a single optimizer.



   .. py:method:: train_step(data: Any) -> Mapping[str, Any]

      The logic for one training step. For more details of the
      implementation, see TensorFlow's documentation of how to
      `customize what happens in Model.fit
      <https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit>`_.



.. py:class:: NatGradWrapper(base_model: gpflow.keras.tf_keras.Model, *args: Any, **kwargs: Any)

   Bases: :py:obj:`NatGradModel`


   Wraps a class-based Keras model (e.g. the return value of
   `gpflux.models.DeepGP.as_training_model`) to make it work with
   `gpflow.optimizers.NaturalGradient` optimizers. For more details, see
   `NatGradModel`.

   (Note that you can also directly pass `NatGradModel` to the
   :class:`~gpflux.models.DeepGP`'s
   :attr:`~gpflux.models.DeepGP.default_model_class` or
   :meth:`~gpflux.models.DeepGP.as_training_model`'s *model_class* arguments.)

   .. todo::

       This class will probably be removed in the future.

   :param base_model: the class-based Keras model to be wrapped


   .. py:method:: call(data: Any, training: Optional[bool] = None) -> Union[tensorflow.Tensor, gpflow.models.model.MeanAndVariance]

      Calls the model on new inputs. Simply passes through to the ``base_model``.



