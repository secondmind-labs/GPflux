{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e09a2c13",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Efficient sampling with Gaussian processes and Random Fourier Features\n",
    "\n",
    "Gaussian processes (GPs) provide a mathematically elegant framework for learning unknown functions from data. They are robust to overfitting, allow to incorporate prior assumptions into the model and provide calibrated uncertainty estimates for their predictions. This makes them prime candidates in settings where data is scarce, noisy or very costly to obtain, and are natural tools in applications such as Bayesian optimisation (BO).\n",
    "\n",
    "Despite their favorable properties, the use of GPs still has practical limitations. One of them is the computational complexity to draw predictive samples from the model, which quickly becomes prohibitive as the sample size grows, and creates a well-known bottleneck for GP-based Thompson sampling (GP-TS) for instance. \n",
    "Recent work <cite data-cite=\"wilson2020efficiently\"/> proposes to combine GPâ€™s weight-space and function-space views to draw samples more efficiently from (approximate) posterior GPs with encouraging results in low-dimensional regimes.\n",
    "\n",
    "In GPflux, this functionality is unlocked by grouping a kernel (e.g., `gpflow.kernels.Matern52`) with its feature decomposition using `gpflux.sampling.KernelWithFeatureDecomposition`. See the notebooks on [weight space approximation](weight_space_approximation.ipynb) and [efficient posterior sampling](efficient_posterior_sampling.ipynb) for a thorough explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9dd12",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gpflow\n",
    "import gpflux\n",
    "\n",
    "from gpflow.config import default_float\n",
    "from gpflow.keras import tf_keras\n",
    "\n",
    "from gpflux.layers.basis_functions.fourier_features import RandomFourierFeaturesCosine\n",
    "from gpflux.sampling import KernelWithFeatureDecomposition\n",
    "from gpflux.models.deep_gp import sample_dgp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1733e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Load Snelson dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4868ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load(\"../../tests/snelson1d.npz\")\n",
    "X, Y = data = d[\"X\"], d[\"Y\"]\n",
    "num_data, input_dim = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af2092",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Setting up the kernel and its feature decomposition\n",
    "\n",
    "The `KernelWithFeatureDecomposition` instance represents a kernel together with its finite feature decomposition,\n",
    "$$\n",
    "k(x, x') = \\sum_{i=0}^L \\lambda_i \\phi_i(x) \\phi_i(x'),\n",
    "$$\n",
    "where $\\lambda_i$ and $\\phi_i(\\cdot)$ are the coefficients (eigenvalues) and features (eigenfunctions), respectively, and $L$ is the finite cutoff. See [the notebook on weight space approximation](weight_space_approximation.ipynb) for a detailed explanation of how to construct this decomposition using Random Fourier Features (RFF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gpflow.kernels.Matern52()\n",
    "Z = np.linspace(X.min(), X.max(), 10).reshape(-1, 1).astype(np.float64)\n",
    "\n",
    "inducing_variable = gpflow.inducing_variables.InducingPoints(Z)\n",
    "gpflow.utilities.set_trainable(inducing_variable, False)\n",
    "\n",
    "num_rff = 1000\n",
    "eigenfunctions = RandomFourierFeaturesCosine(kernel, num_rff, dtype=default_float())\n",
    "eigenvalues = np.ones((num_rff, 1), dtype=default_float())\n",
    "kernel_with_features = KernelWithFeatureDecomposition(kernel, eigenfunctions, eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e15ae9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Building and training the single-layer GP\n",
    "\n",
    "### Initialise the single-layer GP\n",
    "Because `KernelWithFeatureDecomposition` is just a `gpflow.kernels.Kernel`, we can construct a GP layer with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993323c3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "layer = gpflux.layers.GPLayer(\n",
    "    kernel_with_features,\n",
    "    inducing_variable,\n",
    "    num_data,\n",
    "    whiten=True,\n",
    "    num_latent_gps=1,\n",
    "    mean_function=gpflow.mean_functions.Zero(),\n",
    ")\n",
    "likelihood_layer = gpflux.layers.LikelihoodLayer(gpflow.likelihoods.Gaussian())  # noqa: E231\n",
    "dgp = gpflux.models.DeepGP([layer], likelihood_layer)\n",
    "model = dgp.as_training_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925b80f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Fit model to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac422abb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model.compile(tf_keras.optimizers.Adam(learning_rate=0.1))\n",
    "\n",
    "callbacks = [\n",
    "    tf_keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"loss\",\n",
    "        patience=5,\n",
    "        factor=0.95,\n",
    "        verbose=0,\n",
    "        min_lr=1e-6,\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    {\"inputs\": X, \"targets\": Y},\n",
    "    batch_size=num_data,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33701ad2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Drawing samples\n",
    "\n",
    "Now that the model is trained we can draw efficient and consistent samples from the posterior GP. By \"consistent\" we mean that the `sample_dgp` function returns a function object that can be evaluated multiple times at different locations, but importantly, the returned function values will come from the same GP sample. This functionality is implemented by the `gpflux.sampling.efficient_sample` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a44864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "x_margin = 5\n",
    "n_x = 1000\n",
    "X_test = np.linspace(X.min() - x_margin, X.max() + x_margin, n_x).reshape(-1, 1)\n",
    "\n",
    "f_mean, f_var = dgp.predict_f(X_test)\n",
    "f_scale = np.sqrt(f_var)\n",
    "\n",
    "# Plot samples\n",
    "n_sim = 10\n",
    "for _ in range(n_sim):\n",
    "    # `sample_dgp` returns a callable - which we subsequently evaluate\n",
    "    f_sample: Callable[[tf.Tensor], tf.Tensor] = sample_dgp(dgp)\n",
    "    plt.plot(X_test, f_sample(X_test).numpy())\n",
    "\n",
    "# Plot GP mean and uncertainty intervals and data\n",
    "plt.plot(X_test, f_mean, \"C0\")\n",
    "plt.plot(X_test, f_mean + f_scale, \"C0--\")\n",
    "plt.plot(X_test, f_mean - f_scale, \"C0--\")\n",
    "plt.plot(X, Y, \"kx\", alpha=0.2)\n",
    "plt.xlim(X.min() - x_margin, X.max() + x_margin)\n",
    "plt.ylim(Y.min() - x_margin, Y.max() + x_margin)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\"",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
